# -Encoder-Decoder-Models-using-RNN-and-LSTM
 Encoder-Decoder Models using RNN and LSTM and Implementation of Encoder-Decoder using RNN : Visualizing and Enhancing Encoder-Decoder To interpret and improve encoder-decoder outputs with basic attention and 
performance visualization.




Task 1: Conceptual Questions 
1. What is the difference between RNN and LSTM?
anS Recurrent neural network (RNN) is a deep learning model that is trained to process and convert a sequential data input into a specific sequential data output. it is a software system that consists of many interconnected components mimicking how humans perform sequential data conversions, such as translating text from one language to another.
LSTM stands for Long Short-Term Memory and it is a type of neural network and this have 3 types of gate forget gate, input gate, and output gate . 
Forget Gate: Decides what information to discard from the previous state.
Input Gate: Decides what new information to store in the cell state.
utput Gate: Decides what information from the cell state to output.


2  What is the vanishing gradient problem, and how does LSTM solve it?
ans Vanishing gradient problem is a phenomenon that occurs during the training of deep neural networks, where the gradients that are used to update the network become extremely small or "vanish" as they are backpropogated from the output layers to the earlier layers.
During the training process of the neural network, the goal is to minimize a loss function by adjusting the weights of the network.
Batch Normalization : Batch normalization normalizes the inputs of each layer, reducing internal covariate shift. This can help stabilize and accelerate the training process, allowing for more consistent gradient flow
Long Short-Term Memory Networks (LSTMs) and Gated Recurrent Units (GRUs): In the context of recurrent neural networks (RNNs), architectures like LSTMs and GRUs are designed to address the vanishing gradient problem in sequences by incorporating gating mechanisms .
..................................

.....................

.................
.
 there is more content in this raw file
